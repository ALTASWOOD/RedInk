# 本地AI部署指南

## 文档信息

| 项目 | 内容 |
|------|------|
| 文档版本 | V1.0 |
| 创建日期 | 2026-02-28 |

---

## 1. 概述

本地AI是 RedInk 私域环境的核心组件，支持完全离线的AI能力。

### 1.1 支持的本地AI服务

| 服务 | 说明 | 推荐指数 |
|------|------|----------|
| Ollama | 开源本地AI服务，最推荐 | ⭐⭐⭐⭐⭐ |
| LM Studio | GUI界面，适合新手 | ⭐⭐⭐⭐ |
| 自定义API | 兼容OpenAI协议的服务 | ⭐⭐⭐ |

### 1.2 硬件要求

| 模型规模 | 内存要求 | GPU | 推荐配置 |
|----------|----------|-----|----------|
| 7B | 8GB RAM | 可选 | 主流配置 |
| 14B | 16GB RAM | 推荐 | 性能配置 |
| 70B | 64GB+ RAM | 必需 | 高端配置 |

---

## 2. Ollama 部署 (推荐)

### 2.1 Windows 安装

```powershell
# 方法1: 下载安装包
# 访问 https://ollama.ai/download/windows 下载

# 方法2: winget 安装
winget install Ollama.Ollama

# 验证安装
ollama --version
```

### 2.2 macOS 安装

```bash
# 方法1: Homebrew
brew install ollama

# 方法2: 下载安装包
# 访问 https://ollama.ai/download/mac

# 验证安装
ollama --version
```

### 2.3 Linux 安装

```bash
# 一键安装脚本
curl -fsSL https://ollama.ai/install.sh | sh

# 验证安装
ollama --version
```

### 2.4 拉取模型

```bash
# 推荐模型 - 通义千问 (中文优化)
ollama pull qwen:7b          # 7B参数，约4GB
ollama pull qwen:14b         # 14B参数，约8GB

# 其他可选模型
ollama pull llama2:7b        # Meta Llama 2
ollama pull mistral:7b       # Mistral
ollama pull codellama:7b     # 代码优化

# 查看已安装模型
ollama list
```

### 2.5 启动服务

```bash
# 启动 Ollama 服务 (默认端口 11434)
ollama serve

# 后台运行 (Linux/macOS)
nohup ollama serve &

# Windows 服务模式
# Ollama 安装后自动作为服务运行
```

### 2.6 验证服务

```bash
# 检查服务状态
curl http://localhost:11434/api/tags

# 测试模型调用
curl http://localhost:11434/api/generate -d '{
  "model": "qwen:7b",
  "prompt": "你好，请介绍一下自己",
  "stream": false
}'
```

---

## 3. LM Studio 部署

### 3.1 安装

1. 访问 https://lmstudio.ai 下载
2. 运行安装程序
3. 启动 LM Studio

### 3.2 下载模型

1. 打开 LM Studio
2. 点击 "Discover" 搜索模型
3. 搜索 "qwen" 或 "chinese"
4. 下载合适的模型 (推荐 GGUF 格式)

### 3.3 启动服务

1. 点击 "Local Server" 标签
2. 选择已下载的模型
3. 点击 "Start Server"
4. 默认端口: 1234

### 3.4 在 RedInk 中配置

```
服务类型: LM Studio
服务地址: http://localhost:1234
模型名称: 留空 (自动使用当前加载模型)
```

---

## 4. 自定义API部署

### 4.1 协议要求

支持兼容 OpenAI API 协议的服务:

```
POST /v1/chat/completions
{
  "model": "model-name",
  "messages": [...],
  "stream": true/false
}
```

### 4.2 配置示例

```json
{
  "type": "custom",
  "baseUrl": "http://your-server:8000",
  "apiPath": "/v1/chat/completions",
  "model": "your-model-name"
}
```

---

## 5. 性能优化

### 5.1 Ollama 配置优化

```bash
# 设置环境变量
# Linux/macOS: ~/.bashrc 或 ~/.zshrc
# Windows: 系统环境变量

# GPU 加速 (NVIDIA)
export OLLAMA_CUDA_VISIBLE_DEVICES=0

# 调整线程数
export OLLAMA_NUM_PARALLEL=4

# 调整上下文长度
export OLLAMA_NUM_CTX=4096
```

### 5.2 模型选择建议

| 场景 | 推荐模型 | 理由 |
|------|----------|------|
| 日常使用 | qwen:7b | 速度快，质量好 |
| 高质量输出 | qwen:14b | 更好的理解和生成 |
| 快速响应 | qwen:1.8b | 低延迟 |

### 5.3 GPU 加速

```bash
# NVIDIA GPU
# 确保安装 CUDA 驱动
nvidia-smi

# Ollama 自动检测并使用 GPU
```

---

## 6. 网络部署 (多机共享)

### 6.1 服务器端配置

```bash
# 允许外部访问
export OLLAMA_HOST=0.0.0.0:11434
ollama serve
```

### 6.2 防火墙配置

```bash
# Linux
sudo ufw allow 11434/tcp

# Windows
netsh advfirewall firewall add rule name="Ollama" dir=in action=allow protocol=tcp localport=11434
```

### 6.3 客户端配置

在 RedInk 设置中:
```
服务地址: http://ai-server.local:11434
```

---

## 7. 常见问题

| 问题 | 解决方案 |
|------|----------|
| 模型下载慢 | 使用镜像源或代理 |
| 内存不足 | 使用更小的模型 |
| GPU 不可用 | 检查 CUDA 驱动 |
| 连接失败 | 检查服务是否运行 |
| 响应很慢 | 升级硬件或换小模型 |

---

## 8. 验证清单

- [ ] Ollama/LM Studio 已安装
- [ ] 模型已下载
- [ ] 服务已启动
- [ ] `curl localhost:11434/api/tags` 返回正常
- [ ] RedInk 连接测试成功

---

*文档结束*
